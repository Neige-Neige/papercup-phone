"""
MCP Smart Listener Server
ç»“åˆ YAMNet (å£°éŸ³äº‹ä»¶åˆ†ç±») + Whisper (è¯­éŸ³è½¬å½•) + æ‘„åƒå¤´ çš„æ™ºèƒ½æ„ŸçŸ¥æœåŠ¡

åŠŸèƒ½:
1. listen - å®æ—¶å½•éŸ³å¹¶æ™ºèƒ½åˆ†æ
2. classify_sound - ä»…è¯†åˆ«å£°éŸ³ç±»å‹
3. transcribe_speech - ä»…è½¬å½•è¯­éŸ³
4. analyze_file - åˆ†æéŸ³é¢‘æ–‡ä»¶
5. start_monitor - å¯åŠ¨ç¯å¢ƒå¸¸é©»ç›‘å¬
6. stop_monitor - åœæ­¢ç›‘å¬
7. get_monitor_events - è·å–ç›‘å¬åˆ°çš„äº‹ä»¶
8. capture_camera - æ‹ç…§
9. auto_monitor_loop - å¤šæ¨¡æ€ç›‘æ§ï¼ˆå£°éŸ³+å›¾åƒï¼‰
"""

import asyncio
import logging
import tempfile
import os
import threading
import time
from datetime import datetime
from pathlib import Path
from typing import Optional
from collections import deque

import numpy as np
import sounddevice as sd
import soundfile as sf
import tensorflow as tf
import tensorflow_hub as hub
import pandas as pd
import base64

# Whisper
import whisper

# Camera
try:
    import cv2
    CAMERA_AVAILABLE = True
except ImportError:
    CAMERA_AVAILABLE = False
    print("æ³¨æ„: opencv æœªå®‰è£…ï¼Œæ‘„åƒå¤´åŠŸèƒ½ä¸å¯ç”¨ã€‚å®‰è£…: pip install opencv-python")

# MCP
from mcp.server import Server
from mcp.server.stdio import stdio_server
from mcp.types import Tool, TextContent, ImageContent

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("smart-listener")

# ============== æ¨¡å‹åŠ è½½ ==============

print("æ­£åœ¨åŠ è½½ YAMNet æ¨¡å‹...")
yamnet_model = hub.load('https://tfhub.dev/google/yamnet/1')

# åŠ è½½ YAMNet ç±»åˆ«åç§°
class_map_path = yamnet_model.class_map_path().numpy().decode('utf-8')
yamnet_classes = list(pd.read_csv(class_map_path)['display_name'])

print("æ­£åœ¨åŠ è½½ Whisper æ¨¡å‹...")
# å¯é€‰: tiny, base, small, medium, large
# tiny/base é€Ÿåº¦å¿«ï¼Œlarge å‡†ç¡®ç‡é«˜
whisper_model = whisper.load_model("base")

print("æ¨¡å‹åŠ è½½å®Œæˆ!")

# è¯­éŸ³ç›¸å…³çš„ YAMNet ç±»åˆ«ç´¢å¼•
SPEECH_CLASSES = [
    "Speech", "Narration, monologue", "Conversation",
    "Male speech, man speaking", "Female speech, woman speaking",
    "Child speech, kid speaking", "Whispering", "Shout", "Yell",
    "Singing", "Chant"
]



# ============== æ‘„åƒå¤´åŠŸèƒ½ ==============

# ç…§ç‰‡ä¿å­˜ç›®å½•
PHOTO_CACHE_DIR = Path(__file__).parent / "photo_cache"
PHOTO_CACHE_DIR.mkdir(exist_ok=True)

def capture_camera(camera_id: int = 0, warmup_frames: int = 10) -> dict:
    """æ‹ç…§ï¼ˆå¸¦é¢„çƒ­å¸§è®©æ‘„åƒå¤´è‡ªåŠ¨è°ƒæ•´æ›å…‰ï¼‰"""
    if not CAMERA_AVAILABLE:
        return {"error": "opencv æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install opencv-python"}

    try:
        cap = cv2.VideoCapture(camera_id)
        if not cap.isOpened():
            return {"error": f"æ— æ³•æ‰“å¼€æ‘„åƒå¤´ {camera_id}"}

        # é¢„çƒ­ï¼šè¯»å–å‡ å¸§è®©æ‘„åƒå¤´è‡ªåŠ¨è°ƒæ•´æ›å…‰
        for _ in range(warmup_frames):
            cap.read()
            time.sleep(0.05)  # ç¨å¾®ç­‰ä¸€ä¸‹

        # æ­£å¼æ‹ç…§
        ret, frame = cap.read()
        cap.release()

        if not ret:
            return {"error": "æ— æ³•è¯»å–æ‘„åƒå¤´ç”»é¢"}

        # ä¿å­˜åˆ°æœ¬åœ°
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"photo_{timestamp}.jpg"
        filepath = PHOTO_CACHE_DIR / filename
        cv2.imwrite(str(filepath), frame)
        logger.info(f"ç…§ç‰‡å·²ä¿å­˜: {filepath}")

        # è½¬ä¸º JPEG å¹¶ç¼–ç ä¸º base64
        _, buf = cv2.imencode('.jpg', frame, [cv2.IMWRITE_JPEG_QUALITY, 85])
        image_b64 = base64.b64encode(buf).decode()

        return {
            "success": True,
            "image_b64": image_b64,
            "width": frame.shape[1],
            "height": frame.shape[0],
            "saved_path": str(filepath)
        }
    except Exception as e:
        return {"error": str(e)}

# ============== ç¯å¢ƒç›‘å¬çŠ¶æ€ ==============

class EnvironmentMonitor:
    """ç¯å¢ƒå¸¸é©»ç›‘å¬å™¨"""

    def __init__(self):
        self.is_running = False
        self.thread: Optional[threading.Thread] = None
        self.events = deque(maxlen=100)  # æœ€å¤šä¿å­˜100ä¸ªäº‹ä»¶
        self.watch_classes = []  # è¦å…³æ³¨çš„å£°éŸ³ç±»åˆ«
        self.threshold = 0.3  # æ£€æµ‹é˜ˆå€¼
        self.interval = 2.0  # ç›‘å¬é—´éš”(ç§’)
        self.transcribe_speech = True  # æ˜¯å¦è½¬å½•è¯­éŸ³

    def start(self, watch_classes: list[str] = None, threshold: float = 0.3,
              interval: float = 2.0, transcribe: bool = True):
        """å¯åŠ¨ç›‘å¬"""
        if self.is_running:
            return False

        self.watch_classes = watch_classes or []
        self.threshold = threshold
        self.interval = interval
        self.transcribe_speech = transcribe
        self.is_running = True
        self.events.clear()

        self.thread = threading.Thread(target=self._monitor_loop, daemon=True)
        self.thread.start()
        return True

    def stop(self):
        """åœæ­¢ç›‘å¬"""
        self.is_running = False
        if self.thread:
            self.thread.join(timeout=5)
            self.thread = None
        return True

    def _monitor_loop(self):
        """ç›‘å¬å¾ªç¯"""
        logger.info("ç¯å¢ƒç›‘å¬å·²å¯åŠ¨")

        while self.is_running:
            try:
                # å½•åˆ¶ä¸€å°æ®µéŸ³é¢‘
                audio = record_audio(self.interval)

                # åˆ†ç±»
                top_classes, speech_score = classify_with_yamnet(audio)

                # æ£€æŸ¥æ˜¯å¦æœ‰å…³æ³¨çš„å£°éŸ³
                detected = []
                for class_name, score in top_classes:
                    # å¦‚æœæ²¡æœ‰æŒ‡å®šå…³æ³¨ç±»åˆ«ï¼Œæ‰€æœ‰é«˜äºé˜ˆå€¼çš„éƒ½è®°å½•
                    if not self.watch_classes:
                        if score >= self.threshold:
                            detected.append((class_name, score))
                    # å¦‚æœæŒ‡å®šäº†å…³æ³¨ç±»åˆ«ï¼Œåªè®°å½•åŒ¹é…çš„
                    else:
                        for watch in self.watch_classes:
                            if watch.lower() in class_name.lower() and score >= self.threshold:
                                detected.append((class_name, score))
                                break

                # å¦‚æœæ£€æµ‹åˆ°å…³æ³¨çš„å£°éŸ³ï¼Œè®°å½•äº‹ä»¶
                if detected:
                    event = {
                        "time": datetime.now().strftime("%H:%M:%S"),
                        "sounds": detected,
                        "speech_score": speech_score,
                        "transcription": None
                    }

                    # å¦‚æœæ£€æµ‹åˆ°è¯­éŸ³ä¸”éœ€è¦è½¬å½•
                    if self.transcribe_speech and speech_score >= 0.3:
                        try:
                            result = transcribe_with_whisper(audio)
                            if result["text"].strip():
                                event["transcription"] = result
                        except Exception as e:
                            logger.error(f"è½¬å½•å¤±è´¥: {e}")

                    self.events.append(event)
                    logger.info(f"æ£€æµ‹åˆ°äº‹ä»¶: {detected}")

            except Exception as e:
                logger.error(f"ç›‘å¬å¾ªç¯é”™è¯¯: {e}")

            # çŸ­æš‚ä¼‘æ¯é¿å… CPU è¿‡è½½
            time.sleep(0.1)

        logger.info("ç¯å¢ƒç›‘å¬å·²åœæ­¢")

    def get_events(self, clear: bool = False) -> list[dict]:
        """è·å–äº‹ä»¶åˆ—è¡¨"""
        events = list(self.events)
        if clear:
            self.events.clear()
        return events

    def get_status(self) -> dict:
        """è·å–ç›‘å¬çŠ¶æ€"""
        return {
            "is_running": self.is_running,
            "watch_classes": self.watch_classes,
            "threshold": self.threshold,
            "interval": self.interval,
            "event_count": len(self.events)
        }

# å…¨å±€ç›‘å¬å™¨å®ä¾‹
monitor = EnvironmentMonitor()

# ============== å·¥å…·å‡½æ•° ==============

def record_audio(duration: float, sample_rate: int = 16000) -> np.ndarray:
    """å½•åˆ¶éŸ³é¢‘"""
    logger.info(f"å¼€å§‹å½•éŸ³ {duration} ç§’...")
    audio = sd.rec(
        int(duration * sample_rate),
        samplerate=sample_rate,
        channels=1,
        dtype=np.float32
    )
    sd.wait()
    logger.info("å½•éŸ³å®Œæˆ")
    return audio.flatten()


def classify_with_yamnet(audio: np.ndarray) -> tuple[list[tuple[str, float]], float]:
    """
    ä½¿ç”¨ YAMNet åˆ†ç±»å£°éŸ³
    è¿”å›: (top_classes, speech_score)
    """
    scores, embeddings, spectrogram = yamnet_model(audio)
    mean_scores = scores.numpy().mean(axis=0)

    # è·å– top 10 åˆ†ç±»
    top_indices = np.argsort(mean_scores)[-10:][::-1]
    top_classes = [
        (yamnet_classes[idx], float(mean_scores[idx]))
        for idx in top_indices
        if mean_scores[idx] > 0.02
    ]

    # è®¡ç®—è¯­éŸ³å¾—åˆ† - ç´¯åŠ æ‰€æœ‰è¯­éŸ³ç›¸å…³ç±»åˆ«çš„åˆ†æ•°
    speech_score = 0.0
    for class_name in SPEECH_CLASSES:
        if class_name in yamnet_classes:
            idx = yamnet_classes.index(class_name)
            speech_score += mean_scores[idx]

    # ä¹Ÿæ£€æŸ¥ top_classes é‡Œæ˜¯å¦æœ‰è¯­éŸ³ç›¸å…³çš„
    for class_name, score in top_classes:
        for speech_class in SPEECH_CLASSES:
            if speech_class.lower() in class_name.lower():
                speech_score = max(speech_score, score + 0.1)  # åŠ ä¸€ç‚¹æƒé‡
                break

    # é™åˆ¶åœ¨ 0-1 ä¹‹é—´
    speech_score = min(speech_score, 1.0)

    logger.info(f"è¯­éŸ³å¾—åˆ†: {speech_score:.2f}, Top: {top_classes[:3]}")

    return top_classes, speech_score


def transcribe_with_whisper(audio: np.ndarray, sample_rate: int = 16000) -> dict:
    """
    ä½¿ç”¨ Whisper è½¬å½•è¯­éŸ³
    """
    # Whisper éœ€è¦ float32 æ ¼å¼ï¼Œ16kHz
    # ä¿å­˜ä¸ºä¸´æ—¶æ–‡ä»¶
    with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as f:
        sf.write(f.name, audio, sample_rate)
        temp_path = f.name

    try:
        result = whisper_model.transcribe(
            temp_path,
            language=None,  # è‡ªåŠ¨æ£€æµ‹è¯­è¨€
            task="transcribe"
        )
        return {
            "text": result["text"].strip(),
            "language": result.get("language", "unknown")
        }
    finally:
        os.unlink(temp_path)


def format_classifications(classes: list[tuple[str, float]]) -> str:
    """æ ¼å¼åŒ–åˆ†ç±»ç»“æœ"""
    lines = []
    for name, score in classes[:7]:
        bar = "â–ˆ" * int(score * 20)
        lines.append(f"  {name}: {score:.1%} {bar}")
    return "\n".join(lines)


# ============== MCP Server ==============

server = Server("smart-listener")


@server.list_tools()
async def list_tools() -> list[Tool]:
    """åˆ—å‡ºå¯ç”¨å·¥å…·"""
    return [
        Tool(
            name="listen",
            description="æ™ºèƒ½å¬éŸ³ï¼šå½•åˆ¶éŸ³é¢‘ï¼Œè‡ªåŠ¨è¯†åˆ«å£°éŸ³ç±»å‹ã€‚å¦‚æœæ£€æµ‹åˆ°è¯­éŸ³åˆ™åŒæ—¶è½¬å½•å†…å®¹ã€‚",
            inputSchema={
                "type": "object",
                "properties": {
                    "duration": {
                        "type": "number",
                        "description": "å½•éŸ³æ—¶é•¿(ç§’)ï¼Œé»˜è®¤ 5 ç§’",
                        "default": 5
                    },
                    "speech_threshold": {
                        "type": "number",
                        "description": "è¯­éŸ³æ£€æµ‹é˜ˆå€¼ (0-1)ï¼Œé»˜è®¤ 0.15",
                        "default": 0.15
                    }
                }
            }
        ),
        Tool(
            name="classify_sound",
            description="ä»…è¯†åˆ«å£°éŸ³ç±»å‹ï¼ˆä½¿ç”¨ YAMNetï¼‰ï¼Œä¸è½¬å½•è¯­éŸ³å†…å®¹",
            inputSchema={
                "type": "object",
                "properties": {
                    "duration": {
                        "type": "number",
                        "description": "å½•éŸ³æ—¶é•¿(ç§’)",
                        "default": 3
                    }
                }
            }
        ),
        Tool(
            name="transcribe_speech",
            description="ä»…è½¬å½•è¯­éŸ³å†…å®¹ï¼ˆä½¿ç”¨ Whisperï¼‰ï¼Œä¸åˆ†ç±»å£°éŸ³",
            inputSchema={
                "type": "object",
                "properties": {
                    "duration": {
                        "type": "number",
                        "description": "å½•éŸ³æ—¶é•¿(ç§’)",
                        "default": 5
                    }
                }
            }
        ),
        Tool(
            name="analyze_file",
            description="åˆ†æéŸ³é¢‘æ–‡ä»¶ï¼ŒåŒæ—¶è¿›è¡Œå£°éŸ³åˆ†ç±»å’Œè¯­éŸ³è½¬å½•",
            inputSchema={
                "type": "object",
                "properties": {
                    "filepath": {
                        "type": "string",
                        "description": "éŸ³é¢‘æ–‡ä»¶è·¯å¾„ (æ”¯æŒ wav, mp3, flac ç­‰)"
                    }
                },
                "required": ["filepath"]
            }
        ),
        Tool(
            name="list_audio_devices",
            description="åˆ—å‡ºå¯ç”¨çš„éŸ³é¢‘è¾“å…¥è®¾å¤‡",
            inputSchema={
                "type": "object",
                "properties": {}
            }
        ),
        Tool(
            name="start_monitor",
            description="å¯åŠ¨ç¯å¢ƒå¸¸é©»ç›‘å¬ã€‚æŒç»­ç›‘å¬ç¯å¢ƒå£°éŸ³ï¼Œæ£€æµ‹åˆ°æŒ‡å®šç±»å‹çš„å£°éŸ³æ—¶è®°å½•äº‹ä»¶ã€‚",
            inputSchema={
                "type": "object",
                "properties": {
                    "watch_classes": {
                        "type": "array",
                        "items": {"type": "string"},
                        "description": "è¦å…³æ³¨çš„å£°éŸ³ç±»åˆ«åˆ—è¡¨ï¼Œå¦‚ ['dog', 'door', 'speech']ã€‚ç•™ç©ºåˆ™ç›‘å¬æ‰€æœ‰å£°éŸ³ã€‚"
                    },
                    "threshold": {
                        "type": "number",
                        "description": "æ£€æµ‹é˜ˆå€¼ (0-1)ï¼Œé»˜è®¤ 0.3",
                        "default": 0.3
                    },
                    "interval": {
                        "type": "number",
                        "description": "ç›‘å¬é—´éš”(ç§’)ï¼Œé»˜è®¤ 2 ç§’",
                        "default": 2.0
                    },
                    "transcribe": {
                        "type": "boolean",
                        "description": "æ˜¯å¦è½¬å½•æ£€æµ‹åˆ°çš„è¯­éŸ³ï¼Œé»˜è®¤ true",
                        "default": True
                    }
                }
            }
        ),
        Tool(
            name="stop_monitor",
            description="åœæ­¢ç¯å¢ƒå¸¸é©»ç›‘å¬",
            inputSchema={
                "type": "object",
                "properties": {}
            }
        ),
        Tool(
            name="get_monitor_events",
            description="è·å–ç¯å¢ƒç›‘å¬æ£€æµ‹åˆ°çš„äº‹ä»¶åˆ—è¡¨",
            inputSchema={
                "type": "object",
                "properties": {
                    "clear": {
                        "type": "boolean",
                        "description": "è·å–åæ˜¯å¦æ¸…ç©ºäº‹ä»¶åˆ—è¡¨ï¼Œé»˜è®¤ false",
                        "default": False
                    }
                }
            }
        ),
        Tool(
            name="get_monitor_status",
            description="è·å–ç¯å¢ƒç›‘å¬å™¨çš„å½“å‰çŠ¶æ€",
            inputSchema={
                "type": "object",
                "properties": {}
            }
        ),
        Tool(
            name="capture_camera",
            description="æ‹ä¸€å¼ ç…§ç‰‡è®© Claude åˆ†æï¼ˆè¡¨æƒ…ã€åŠ¨ä½œã€ç‰©ä½“ã€åœºæ™¯ï¼‰ğŸ“·",
            inputSchema={
                "type": "object",
                "properties": {
                    "camera_id": {
                        "type": "number",
                        "description": "æ‘„åƒå¤´ IDï¼Œé»˜è®¤ 0ï¼ˆå†…ç½®æ‘„åƒå¤´ï¼‰",
                        "default": 0
                    }
                }
            }
        ),
        Tool(
            name="auto_monitor_loop",
            description="""ã€å¤šæ¨¡æ€ç›‘æ§ã€‘å¬åˆ°å£°éŸ³æ—¶è‡ªåŠ¨æ‹ç…§ï¼ğŸ¤ğŸ“·

æ£€æµ‹åˆ°å£°éŸ³ â†’ è‡ªåŠ¨æ‹ç…§ â†’ è¿”å›å£°éŸ³ç±»å‹ + ç”»é¢
ä¾‹å¦‚ï¼šå¬åˆ°å’³å—½ + çœ‹åˆ°æ‰çœ¼ç› â†’ "çœ‹èµ·æ¥ç´¯äº†ï¼Œä¼‘æ¯ä¸€ä¸‹ï¼Ÿ"

å“åº”åè®°å¾—ç»§ç»­è°ƒç”¨è¿™ä¸ªå·¥å…·æ¥ä¿æŒç›‘æ§ï¼""",
            inputSchema={
                "type": "object",
                "properties": {
                    "max_wait_seconds": {
                        "type": "number",
                        "description": "æœ€å¤§ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤ 60",
                        "default": 60
                    },
                    "sensitivity": {
                        "type": "number",
                        "description": "æ£€æµ‹çµæ•åº¦ (0-1)ï¼Œè¶Šä½è¶Šæ•æ„Ÿï¼Œé»˜è®¤ 0.2",
                        "default": 0.2
                    },
                    "camera_id": {
                        "type": "number",
                        "description": "æ‘„åƒå¤´ IDï¼Œé»˜è®¤ 0",
                        "default": 0
                    }
                }
            }
        ),
        Tool(
            name="listen_and_respond",
            description="""ç­‰å¾…å¹¶ç›‘å¬æœ‰æ„ä¹‰çš„å£°éŸ³ï¼Œè¿”å›å£°éŸ³ç±»å‹å’Œå“åº”å»ºè®®ã€‚

å¿½ç•¥é™éŸ³å’Œå™ªéŸ³ï¼Œåªåœ¨æ£€æµ‹åˆ°æœ‰æ„ä¹‰çš„å£°éŸ³æ—¶è¿”å›ã€‚
ä¾‹å¦‚ï¼šå’³å—½ã€ç¬‘å£°ã€æ•²é—¨ã€ç‹—å«ç­‰ã€‚""",
            inputSchema={
                "type": "object",
                "properties": {
                    "max_wait_seconds": {
                        "type": "number",
                        "description": "æœ€å¤§ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤ 30",
                        "default": 30
                    },
                    "sensitivity": {
                        "type": "number",
                        "description": "æ£€æµ‹çµæ•åº¦ (0-1)ï¼Œé»˜è®¤ 0.25",
                        "default": 0.25
                    }
                }
            }
        )
    ]


@server.call_tool()
async def call_tool(name: str, arguments: dict) -> list[TextContent]:
    """å¤„ç†å·¥å…·è°ƒç”¨"""

    if name == "listen":
        return await smart_listen(
            duration=arguments.get("duration", 5),
            speech_threshold=arguments.get("speech_threshold", 0.3)
        )

    elif name == "classify_sound":
        return await classify_sound_only(
            duration=arguments.get("duration", 3)
        )

    elif name == "transcribe_speech":
        return await transcribe_speech_only(
            duration=arguments.get("duration", 5)
        )

    elif name == "analyze_file":
        return await analyze_audio_file(
            filepath=arguments["filepath"]
        )

    elif name == "list_audio_devices":
        return await list_audio_devices()

    elif name == "start_monitor":
        return await start_environment_monitor(
            watch_classes=arguments.get("watch_classes", []),
            threshold=arguments.get("threshold", 0.3),
            interval=arguments.get("interval", 2.0),
            transcribe=arguments.get("transcribe", True)
        )

    elif name == "stop_monitor":
        return await stop_environment_monitor()

    elif name == "get_monitor_events":
        return await get_monitor_events(
            clear=arguments.get("clear", False)
        )

    elif name == "get_monitor_status":
        return await get_monitor_status()

    elif name == "capture_camera":
        return await capture_camera_tool(
            camera_id=int(arguments.get("camera_id", 0))
        )

    elif name == "auto_monitor_loop":
        return await auto_monitor_loop(
            max_wait_seconds=arguments.get("max_wait_seconds", 60),
            sensitivity=arguments.get("sensitivity", 0.2),
            camera_id=int(arguments.get("camera_id", 0))
        )

    elif name == "listen_and_respond":
        return await listen_and_respond(
            max_wait_seconds=arguments.get("max_wait_seconds", 30),
            sensitivity=arguments.get("sensitivity", 0.25)
        )

    else:
        return [TextContent(type="text", text=f"æœªçŸ¥å·¥å…·: {name}")]


async def smart_listen(duration: float, speech_threshold: float) -> list[TextContent]:
    """æ™ºèƒ½å¬éŸ³ï¼šç»“åˆ YAMNet + Whisper"""

    # åœ¨çº¿ç¨‹æ± ä¸­è¿è¡Œå½•éŸ³ï¼ˆé¿å…é˜»å¡ï¼‰
    loop = asyncio.get_event_loop()
    audio = await loop.run_in_executor(None, record_audio, duration)

    # YAMNet åˆ†ç±»
    top_classes, speech_score = await loop.run_in_executor(
        None, classify_with_yamnet, audio
    )

    result_lines = ["ğŸ§ æ™ºèƒ½å¬éŸ³åˆ†æç»“æœ", "=" * 30, ""]

    # å£°éŸ³åˆ†ç±»ç»“æœ
    result_lines.append("ğŸ“Š æ£€æµ‹åˆ°çš„å£°éŸ³ç±»å‹:")
    result_lines.append(format_classifications(top_classes))
    result_lines.append("")
    result_lines.append(f"ğŸ—£ï¸ è¯­éŸ³æ£€æµ‹å¾—åˆ†: {speech_score:.1%} (é˜ˆå€¼: {speech_threshold:.1%})")
    result_lines.append("")

    # å¦‚æœæ£€æµ‹åˆ°è¯­éŸ³ï¼Œè¿›è¡Œè½¬å½•
    if speech_score >= speech_threshold:
        result_lines.append("âœ… æ£€æµ‹åˆ°è¯­éŸ³ï¼Œæ­£åœ¨è½¬å½•...")

        transcription = await loop.run_in_executor(
            None, transcribe_with_whisper, audio
        )

        result_lines.append("")
        result_lines.append(f"ğŸ“ è½¬å½•ç»“æœ [{transcription['language']}]:")
        result_lines.append(f"   \"{transcription['text']}\"")
    else:
        result_lines.append("â„¹ï¸ æœªæ£€æµ‹åˆ°æ˜æ˜¾è¯­éŸ³ï¼Œè·³è¿‡è½¬å½•")

    return [TextContent(type="text", text="\n".join(result_lines))]


async def classify_sound_only(duration: float) -> list[TextContent]:
    """ä»…è¿›è¡Œå£°éŸ³åˆ†ç±»"""

    loop = asyncio.get_event_loop()
    audio = await loop.run_in_executor(None, record_audio, duration)

    top_classes, speech_score = await loop.run_in_executor(
        None, classify_with_yamnet, audio
    )

    result_lines = ["ğŸ”Š å£°éŸ³åˆ†ç±»ç»“æœ (YAMNet)", "=" * 30, ""]
    result_lines.append(format_classifications(top_classes))
    result_lines.append("")
    result_lines.append(f"è¯­éŸ³æ£€æµ‹å¾—åˆ†: {speech_score:.1%}")

    return [TextContent(type="text", text="\n".join(result_lines))]


async def transcribe_speech_only(duration: float) -> list[TextContent]:
    """ä»…è¿›è¡Œè¯­éŸ³è½¬å½•"""

    loop = asyncio.get_event_loop()
    audio = await loop.run_in_executor(None, record_audio, duration)

    transcription = await loop.run_in_executor(
        None, transcribe_with_whisper, audio
    )

    result_lines = [
        "ğŸ¤ è¯­éŸ³è½¬å½•ç»“æœ (Whisper)",
        "=" * 30,
        "",
        f"æ£€æµ‹è¯­è¨€: {transcription['language']}",
        "",
        "è½¬å½•å†…å®¹:",
        f"\"{transcription['text']}\""
    ]

    return [TextContent(type="text", text="\n".join(result_lines))]


async def analyze_audio_file(filepath: str) -> list[TextContent]:
    """åˆ†æéŸ³é¢‘æ–‡ä»¶"""

    path = Path(filepath)
    if not path.exists():
        return [TextContent(type="text", text=f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")]

    # åŠ è½½éŸ³é¢‘æ–‡ä»¶
    try:
        import librosa
        audio, sr = librosa.load(str(path), sr=16000, mono=True)
    except Exception as e:
        return [TextContent(type="text", text=f"âŒ æ— æ³•åŠ è½½éŸ³é¢‘æ–‡ä»¶: {e}")]

    duration = len(audio) / 16000

    result_lines = [
        f"ğŸ“ éŸ³é¢‘æ–‡ä»¶åˆ†æ: {path.name}",
        f"   æ—¶é•¿: {duration:.1f} ç§’",
        "=" * 40,
        ""
    ]

    loop = asyncio.get_event_loop()

    # YAMNet åˆ†ç±»
    top_classes, speech_score = await loop.run_in_executor(
        None, classify_with_yamnet, audio
    )

    result_lines.append("ğŸ“Š å£°éŸ³ç±»å‹:")
    result_lines.append(format_classifications(top_classes))
    result_lines.append("")

    # Whisper è½¬å½•
    if speech_score > 0.2:
        result_lines.append(f"ğŸ—£ï¸ æ£€æµ‹åˆ°è¯­éŸ³ ({speech_score:.1%})")

        transcription = await loop.run_in_executor(
            None, transcribe_with_whisper, audio
        )

        result_lines.append(f"ğŸ“ è½¬å½• [{transcription['language']}]:")
        result_lines.append(f"   \"{transcription['text']}\"")
    else:
        result_lines.append("â„¹ï¸ æœªæ£€æµ‹åˆ°æ˜æ˜¾è¯­éŸ³å†…å®¹")

    return [TextContent(type="text", text="\n".join(result_lines))]


async def list_audio_devices() -> list[TextContent]:
    """åˆ—å‡ºéŸ³é¢‘è®¾å¤‡"""

    devices = sd.query_devices()
    input_devices = []

    for i, dev in enumerate(devices):
        if dev['max_input_channels'] > 0:
            default = " (é»˜è®¤)" if i == sd.default.device[0] else ""
            input_devices.append(f"  [{i}] {dev['name']}{default}")

    result = "ğŸ™ï¸ å¯ç”¨éŸ³é¢‘è¾“å…¥è®¾å¤‡:\n" + "\n".join(input_devices)
    return [TextContent(type="text", text=result)]


# ============== ç¯å¢ƒç›‘å¬åŠŸèƒ½ ==============

async def start_environment_monitor(
    watch_classes: list[str],
    threshold: float,
    interval: float,
    transcribe: bool
) -> list[TextContent]:
    """å¯åŠ¨ç¯å¢ƒç›‘å¬"""

    if monitor.is_running:
        return [TextContent(type="text", text="âš ï¸ ç›‘å¬å™¨å·²ç»åœ¨è¿è¡Œä¸­")]

    success = monitor.start(
        watch_classes=watch_classes,
        threshold=threshold,
        interval=interval,
        transcribe=transcribe
    )

    if success:
        watch_info = ", ".join(watch_classes) if watch_classes else "æ‰€æœ‰å£°éŸ³"
        result_lines = [
            "ğŸ§ ç¯å¢ƒç›‘å¬å·²å¯åŠ¨!",
            "=" * 30,
            f"ğŸ“Œ ç›‘å¬ç›®æ ‡: {watch_info}",
            f"ğŸ“Š æ£€æµ‹é˜ˆå€¼: {threshold:.0%}",
            f"â±ï¸ ç›‘å¬é—´éš”: {interval} ç§’",
            f"ğŸ—£ï¸ è¯­éŸ³è½¬å½•: {'å¼€å¯' if transcribe else 'å…³é—­'}",
            "",
            "ğŸ’¡ ä½¿ç”¨ get_monitor_events æŸ¥çœ‹æ£€æµ‹åˆ°çš„äº‹ä»¶",
            "ğŸ’¡ ä½¿ç”¨ stop_monitor åœæ­¢ç›‘å¬"
        ]
        return [TextContent(type="text", text="\n".join(result_lines))]
    else:
        return [TextContent(type="text", text="âŒ å¯åŠ¨ç›‘å¬å¤±è´¥")]


async def stop_environment_monitor() -> list[TextContent]:
    """åœæ­¢ç¯å¢ƒç›‘å¬"""

    if not monitor.is_running:
        return [TextContent(type="text", text="âš ï¸ ç›‘å¬å™¨æ²¡æœ‰åœ¨è¿è¡Œ")]

    event_count = len(monitor.events)
    monitor.stop()

    return [TextContent(type="text", text=f"ğŸ›‘ ç¯å¢ƒç›‘å¬å·²åœæ­¢\nğŸ“Š å…±è®°å½•äº† {event_count} ä¸ªäº‹ä»¶")]


async def get_monitor_events(clear: bool) -> list[TextContent]:
    """è·å–ç›‘å¬äº‹ä»¶"""

    events = monitor.get_events(clear=clear)

    if not events:
        status = "è¿è¡Œä¸­" if monitor.is_running else "å·²åœæ­¢"
        return [TextContent(type="text", text=f"ğŸ“­ æš‚æ— äº‹ä»¶ (ç›‘å¬å™¨çŠ¶æ€: {status})")]

    result_lines = [
        f"ğŸ“‹ æ£€æµ‹åˆ°çš„äº‹ä»¶ ({len(events)} ä¸ª)",
        "=" * 40,
        ""
    ]

    for i, event in enumerate(events, 1):
        result_lines.append(f"ğŸ• [{event['time']}] äº‹ä»¶ #{i}")

        # å£°éŸ³ç±»å‹
        sounds = ", ".join([f"{name} ({score:.0%})" for name, score in event['sounds']])
        result_lines.append(f"   ğŸ”Š å£°éŸ³: {sounds}")

        # è¯­éŸ³è½¬å½•
        if event.get('transcription'):
            trans = event['transcription']
            result_lines.append(f"   ğŸ—£ï¸ è¯­éŸ³ [{trans['language']}]: \"{trans['text']}\"")

        result_lines.append("")

    if clear:
        result_lines.append("âœ… äº‹ä»¶åˆ—è¡¨å·²æ¸…ç©º")

    return [TextContent(type="text", text="\n".join(result_lines))]


async def get_monitor_status() -> list[TextContent]:
    """è·å–ç›‘å¬çŠ¶æ€"""

    status = monitor.get_status()

    watch_info = ", ".join(status['watch_classes']) if status['watch_classes'] else "æ‰€æœ‰å£°éŸ³"
    running_emoji = "ğŸŸ¢" if status['is_running'] else "ğŸ”´"

    result_lines = [
        "ğŸ“Š ç¯å¢ƒç›‘å¬å™¨çŠ¶æ€",
        "=" * 30,
        f"{running_emoji} çŠ¶æ€: {'è¿è¡Œä¸­' if status['is_running'] else 'å·²åœæ­¢'}",
        f"ğŸ“Œ ç›‘å¬ç›®æ ‡: {watch_info}",
        f"ğŸ“Š æ£€æµ‹é˜ˆå€¼: {status['threshold']:.0%}",
        f"â±ï¸ ç›‘å¬é—´éš”: {status['interval']} ç§’",
        f"ğŸ“ å·²è®°å½•äº‹ä»¶: {status['event_count']} ä¸ª"
    ]

    return [TextContent(type="text", text="\n".join(result_lines))]


# ============== æ‘„åƒå¤´å’Œå¤šæ¨¡æ€åŠŸèƒ½ ==============

async def capture_camera_tool(camera_id: int) -> list:
    """æ‹ç…§å·¥å…·"""
    result = capture_camera(camera_id)

    if "error" in result:
        return [TextContent(type="text", text=f"âŒ æ‹ç…§å¤±è´¥: {result['error']}")]

    return [
        ImageContent(type="image", data=result["image_b64"], mimeType="image/jpeg"),
        TextContent(type="text", text=f"ğŸ“· å·²æ‹æ‘„ ({result['width']}x{result['height']})")
    ]


async def auto_monitor_loop(max_wait_seconds: float, sensitivity: float, camera_id: int) -> list:
    """å¤šæ¨¡æ€ç›‘æ§ï¼šæ£€æµ‹åˆ°å£°éŸ³æ—¶è‡ªåŠ¨æ‹ç…§"""

    ignore_classes = {"Silence", "White noise", "Pink noise", "Static"}
    segment_duration = 2
    start_time = time.time()

    while time.time() - start_time < max_wait_seconds:
        try:
            # å½•åˆ¶éŸ³é¢‘
            loop = asyncio.get_event_loop()
            audio = await loop.run_in_executor(None, record_audio, segment_duration)

            # åˆ†æ
            top_classes, speech_score = await loop.run_in_executor(
                None, classify_with_yamnet, audio
            )

            # è¿‡æ»¤æ‰å™ªéŸ³
            meaningful = [
                (name, score) for name, score in top_classes
                if name not in ignore_classes and score >= sensitivity
            ]

            if meaningful:
                top_sound = meaningful[0][0]
                sounds_str = ", ".join(f"{name} ({score:.0%})" for name, score in meaningful[:3])

                # æ‹ç…§
                cam_result = capture_camera(camera_id)

                result_text = f"""ğŸ¤ æ£€æµ‹åˆ°å£°éŸ³!
{'=' * 35}
ğŸ”Š å£°éŸ³: {sounds_str}
ğŸ† ä¸»è¦å£°éŸ³: {top_sound}

è¯·æ ¹æ®å£°éŸ³å’Œç”»é¢åšå‡ºå›åº”ï¼Œç„¶åç»§ç»­è°ƒç”¨ auto_monitor_loop ä¿æŒç›‘æ§ï¼"""

                if "error" not in cam_result:
                    return [
                        ImageContent(type="image", data=cam_result["image_b64"], mimeType="image/jpeg"),
                        TextContent(type="text", text=result_text)
                    ]
                else:
                    return [TextContent(type="text", text=result_text + f"\n\n(æ‹ç…§å¤±è´¥: {cam_result['error']})")]

            await asyncio.sleep(0.2)

        except Exception as e:
            logger.error(f"ç›‘æ§å¾ªç¯é”™è¯¯: {e}")
            return [TextContent(type="text", text=f"âŒ é”™è¯¯: {e}")]

    return [TextContent(type="text", text=f"ğŸ”‡ å®‰é™äº† {max_wait_seconds} ç§’ï¼Œç»§ç»­è°ƒç”¨ auto_monitor_loop ä¿æŒç›‘æ§")]


async def listen_and_respond(max_wait_seconds: float, sensitivity: float) -> list[TextContent]:
    """ç­‰å¾…å¹¶ç›‘å¬æœ‰æ„ä¹‰çš„å£°éŸ³ï¼Œè¿”å›å£°éŸ³ç±»å‹å’Œå“åº”å»ºè®®"""

    ignore_classes = {"Silence", "White noise", "Pink noise", "Static"}
    segment_duration = 2
    start_time = time.time()

    while time.time() - start_time < max_wait_seconds:
        try:
            loop = asyncio.get_event_loop()
            audio = await loop.run_in_executor(None, record_audio, segment_duration)

            top_classes, speech_score = await loop.run_in_executor(
                None, classify_with_yamnet, audio
            )

            meaningful = [
                (name, score) for name, score in top_classes
                if name not in ignore_classes and score >= sensitivity
            ]

            if meaningful:
                top_sound = meaningful[0][0]
                wait_time = round(time.time() - start_time, 1)
                sounds_str = "\n".join(f"  - {name} ({score:.0%})" for name, score in meaningful)

                result_lines = [
                    "ğŸ§ æ£€æµ‹åˆ°å£°éŸ³!",
                    "=" * 35,
                    f"â±ï¸ ç­‰å¾…æ—¶é—´: {wait_time} ç§’",
                    "",
                    "ğŸ”Š æ£€æµ‹åˆ°çš„å£°éŸ³:",
                    sounds_str,
                    "",
                    f"ğŸ† ä¸»è¦å£°éŸ³: {top_sound}",
                ]

                # å¦‚æœæœ‰è¯­éŸ³ï¼Œå°è¯•è½¬å½•
                if speech_score >= 0.15:
                    transcription = await loop.run_in_executor(
                        None, transcribe_with_whisper, audio
                    )
                    if transcription["text"].strip():
                        result_lines.append("")
                        result_lines.append(f"ğŸ—£ï¸ è¯­éŸ³å†…å®¹: \"{transcription['text']}\"")

                return [TextContent(type="text", text="\n".join(result_lines))]

            await asyncio.sleep(0.2)

        except Exception as e:
            return [TextContent(type="text", text=f"âŒ ç›‘å¬é”™è¯¯: {e}")]

    return [TextContent(type="text", text=f"ğŸ”‡ ç›‘å¬äº† {max_wait_seconds} ç§’ - ç¯å¢ƒå¾ˆå®‰é™")]


# ============== ä¸»å…¥å£ ==============

async def main():
    """å¯åŠ¨ MCP æœåŠ¡å™¨"""
    logger.info("å¯åŠ¨ Smart Listener MCP æœåŠ¡å™¨...")

    async with stdio_server() as (read_stream, write_stream):
        await server.run(
            read_stream,
            write_stream,
            server.create_initialization_options()
        )


if __name__ == "__main__":
    asyncio.run(main())
